#Overview

What we want to archive is visualising our logs, not only use command to find, just using keywords to search. 
We found ELK is a very impressive solution.

##Architecture

ELKR

![Logstash](http://logstash.net/docs/1.1.1/tutorials/getting-started-centralized-overview-diagram.png)


###Elasticsearch

Elasticsearch is a search server based on Lucene. It provides a distributed, multitenant-capable full-text search engine with a RESTful web interface and schema-free JSON documents. Elasticsearch is developed in Java and is released as open source under the terms of the Apache License.

###Logstash

logstash is a tool for managing events and logs. You can use it to collect logs, parse them, and store them for later use (like, for searching). Speaking of searching, logstash comes with a web interface for searching and drilling into all of your logs.

###Kibana

Kibana is a great tool for real time data analytics and simply being on this page has put you on your way to making the most of it! If you have not downloaded Kibana yet, you can get it here: Download Kibana. We recommend you start this tutorial with a clean Elasticsearch instance.

###Redis

Redis is an open source, BSD licensed, advanced key-value cache and store. It is often referred to as a data structure server since keys can contain strings, hashes, lists, sets, sorted sets, bitmaps and hyperloglogs

###Logstash-forwarder

A tool to collect logs locally in preparation for processing elsewhere!


##Setup

###Prerequisites
Monitor Server :10.18.10.2

Client Server: 10.19.2.179

OS: CentOS 6

###Install(ELK+R)

Install Java

`sudo yum install -y java-1.7.0-openjdk`

Install Logstash

```bash
cat > /etc/yum.repos.d/logstash << EOF
[logstash-1.4]
name=logstash
baseurl=http://packages.elasticsearch.org/logstash/1.4/centos
gpgcheck=1
gpgkey=http://packages.elasticsearch.org/GPG-KEY-elasticsearch
enabled=1
EOF
yum install -y logstash
```

Install Elasticsearch

```bash
sudo cat > /etc/yum.repos.d/elasticsearch.repo << EOF
[elasticsearch-1.1]
name=Elasticsearch repository for 1.1.x packages
baseurl=http://packages.elasticsearch.org/elasticsearch/1.1/centos
gpgcheck=1
gpgkey=http://packages.elasticsearch.org/GPG-KEY-elasticsearch
enabled=1
EOF
sudo yum install -y elasticsearch
sudo service elasticsearch restart
sudo /sbin/chkconfig --add elasticsearch
```

Install Kibana

```bash
cd ~; curl -O https://download.elasticsearch.org/kibana/kibana/kibana-3.0.1.tar.gz
tar xvf kibana-3.0.1.tar.gz
sudo mkdir -p /usr/share/nginx/kibana3
sudo cp -R ~/kibana-3.0.1/* /usr/share/nginx/kibana3/
```


We need install nginx to host Kibana

```bash
sudo rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm
yum install -y nginx
```

Due to Kibana already provide nginx conf, we can download it and place it

```bash
cd ~; curl -OL https://gist.githubusercontent.com/thisismitch/2205786838a6a5d61f55/raw/f91e06198a7c455925f6e3099e3ea7c186d0b263/nginx.conf
cat >> nginx.conf <<EOF
server_name FQDN;
root /usr/share/nginx/kibana3
sudo cp ~/nginx.conf /etc/nginx/conf.d/default.conf
```

>If you are not using htpasswd as authentication, we can mark the auth_basic

```bash
sudo service nginx restart
sudo chkconfig --levels 235 nginx on
```

>If you are using Kibana3 + Elasticsearch1.4 you need to few steps before complete settings

```bash
sudo vi /etc/elasticsearch/elasticsearch.yml
script.disable_dynamic: true
http.cors.allow-origin: "/.*/"
http.cors.allow-credentials: true
http.cors.enabled: true
```

Install Logstash

```bash
sudo cat > /etc/yum.repos.d/logstash.repo << EOF
[logstash-1.4]
name=logstash
baseurl=http://packages.elasticsearch.org/logstash/1.4/centos
gpgcheck=1
gpgkey=http://packages.elasticsearch.org/GPG-KEY-elasticsearch
enabled=1
EOF
sudo yum install -y logstash
```

Install Redis
Here we are using the default and single node redis settings

```bash
sudo yum install -y redis
sudo service redis restart
```

Now We are fully implement applications, we can start our first log.
We are picking up the Jenkins job build output as our samples.

But first you need to have a Jenkins CI. We assume they are running with our ELK monitor server is separate servers.

Let's launch the jenkins.

`java -jar jenkins.war`

Install [logstash](https://wiki.jenkins-ci.org/display/JENKINS/Logstash+Plugin) plugin. After installed, configure the plugin with your environment.
![plugin](https://github.com/leitu/leitu.github.com/blob/master/_pics/logstash%2Bredis.png)
we also need to check the send console log to logstash
![console](https://github.com/leitu/leitu.github.com/blob/master/_pics/jenkins_jobs_settings.png)


After running few builds job, you can verify the logs in your redis server

```bash
sudo redis-cli
redis 127.0.0.1:6379> llen logstash
(integer) 1
redis 127.0.0.1:6379> lpop logstash
"id":"2015-01-05_16-10-56","projectName":"test","dis"
redis 127.0.0.1:6379> llen logstash
(integer) 0
redis 127.0.0.1:6379>exit
```

you will see the logs had been shipper to redis.

How we transfer all the redis logs to indexer?

Let' configure on monitor server

```json
sudo cat > /etc/logstash/conf.d/01-jenkins.conf << EOF

input {
  redis {
    host => "10.18.10.2"
      port => "6380"
      key  => "logstash"
      data_type => "list"
      codec  => json
  }
}
output {
  elasticsearch {
    host => localhost
      embedded => true
  }
  stdout { codec => rubydebug }
}
EOF
sudo service logstash restart
```


*This is JSON format, we highly recommend that you are verify the writings before you use this configure file.
The easiest way is to run /opt/logstash/bin/logstash -f /etc/logstash/conf.d/shiper.conf -t or verify the json code only with [jsonlint](http://jsonlint.com/)*

We are not adding any filter here.

Then you will see a beautiful graph on your kibana
![Kibana](https://github.com/leitu/leitu.github.com/blob/master/_pics/Kibana_jenkins.png)



What if we want to monitor server log?
Install logstash on the target server according to above installation guide.

Let's choose below logs as what samples.
- /var/log/message
- /var/log/secure

We create /etc/logstash/conf.d/shipper.conf as flowing content in target machine.

```json
sudo cat > /etc/logstash/conf.d/shipper.conf << EOF
input {
  file {
    type => "syslog"
      path => ["/var/log/secure", "/var/log/syslog"]
  }
}
filter {
  mutate {
    replace => ["host", "SHIPPER_HOSTNAME"]
  }
}
output {
  redis {
    host => "10.18.10.2"
      data_type => "list"
      key => "logstash"
      codec => json
  }
}
EOF
sudo service logstash restart
```

*Again, verify your code before restart the process*

It's pretty cool, uh?

Let's do something different, here we would use [logstash-forwarder](https://github.com/elasticsearch/logstash-forwarder)

##Logstash-forwarder
Reids is very good, and lightwight. but why we need logstash-forwarder here.

>Redis provides simple authentication but no transport-layer encryption or authorization. This is >perfectly fine in trusted environments. However, if you're connecting to Redis between datacenters you >will probably want to use encryption.

Logstash-forwarder is using openssl to encrypt, perviously it's called lumberjack, and that's why in the config file it called lumberjack.

On the monitor server side, we need to generate certificate

```json
cd /etc/pki/tls; sudo openssl req -x509 -batch -nodes -days 3650 -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt
scp private/logstash-forwarder.key root@target_server_ip:/etc/pki/tls/private
sudo cat > /etc/logstash/conf.d/02-lumberjack-input.conf <<EOF
input {
  lumberjack {
    port => 5000
      type => "logs"
      ssl_certificate => "/etc/pki/tls/certs/logstash-forwarder.crt"
      ssl_key => "/etc/pki/tls/private/logstash-forwarder.key"
  }
}
EOF
sudo cat > /etc/logstash/conf.d/03-syslog.conf <<EOF
filter {
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_ho
        stname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}
        " }
        add_field => [ "received_at", "%{@timestamp}" ]
          add_field => [ "received_from", "%{host}" ]
    }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        remove_field => ["timestamp"]
    }
    date {
      match => ["timestamp8601", "ISO8601"]
        remove_field => ["timestamp8601"]
    }
  }
}
EOF
```

*Here we are using filter, the as some default define by logstash team such as SYSLOGTIMESTAMP. in the latest version, the definition of the core patterns are now provided by the logstash-patterns-core gem.
They empty this folder, which can let users to use for custom patterns.*

```json
sudo cat > /etc/logstash/conf.d/30-lumberjack-output.conf <<EOF
output {
  elasticsearch {
    host => "localhost"
      port => "9200"
      protocol => "http"
      index => "logstash-%{+YYYY.MM.dd}-%{type}"
  }
  stdout { codec => rubydebug }
}
EOF
sudo service logstash restart
```


Voila. The monitor server is ready to use.

Now let's switch back to target server, in the above we already copy the key to the target server.

```json
sudo cat > /etc/yum.repos.d/logstash-forwarder.repo << EOF
[logstash-forwarder]
name=logstash-forwarder
baseurl=http://packages.elasticsearch.org/logstash-forwarder/centos
gpgcheck=1
gpgkey=http://packages.elasticsearch.org/GPG-KEY-elasticsearch
enabled=1
EOF
sudo yum install -y logstash-forwarder
```

*you can curl my modified [logstash-forwader](https://gist.github.com/ae30a4c1a1f342df1274.git) init script or use the original one.*


```json
sudo cat > /etc/logstash-forwarder << EOF
{
  "network": {
    "servers": [ "10.18.10.2:5000" ],
      "timeout": 15,
      "ssl ca": "/etc/pki/tls/certs/logstash-forwarder.crt"
  },
    "files": [
    {
      "paths": [
        "/var/log/message",
      "/var/log/secure"
        ],
      "fields": { "type": "syslog" }
    }
  ]
}
EOF
sudo service logstash-forwarder start
```

Now you can see your graphic logs.

Still keep in mind


>Warning
>It's probably not a good idea to use the certificates in this manner for production environments. If an attacker can get the certificate from any shipper, they can impersonate the indexer. This can be an issue when shipping logs containing sensitive information. You should generate a separate certificate for the indexer.


P.S. All above settings are based on single elasticsearch plus single logstash indexer and single redis. If you want to setup cluster or more nodes, we will post it later

Refer:
[1][digtialocean](https://www.digitalocean.com/community/tutorials/how-to-use-logstash-and-kibana-to-centralize-logs-on-centos-6#SetUpLogstashForwarder)
[2][Michael](http://michael.bouvy.net/blog/en/2013/11/19/collect-visualize-your-logs-logstash-elasticsearch-redis-kibana/)
[3][Ian](https://ianunruh.com/2014/05/monitor-everything-part-2.html)

